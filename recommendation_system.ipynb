{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbd7356",
   "metadata": {},
   "source": [
    "# Anime Recommendation System: Introduction\n",
    "\n",
    "## Objective\n",
    "The goal of this project is to build a **recommendation system** that can suggest anime to users based on their past ratings and anime characteristics.\n",
    "\n",
    "## Dataset Overview\n",
    "We are using the [Anime Dataset](#https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database?select=rating.csv) containing:\n",
    "- **Anime information**: anime_id,name,genre,type,episodes,rating,members.\n",
    "- **User ratings**: user_id,anime_id,rating.\n",
    "\n",
    "## Project Goals\n",
    "1. Explore and preprocess the dataset to handle sparsity, missing values and noisy ratings.\n",
    "2. Implement multiple recommendation techniques:\n",
    "    - **Baseline recommendation**: Simple popularity-based recommendations.\n",
    "    - **Collaborative Filtering (CF)**: Suggest anime based on user-user or item-item similarity.\n",
    "    - **Content-Based Filtering (CBF)**: Recommend anime using genre, type, and other features.\n",
    "    - **Hybrid approaches**: Combine CF and CBF for better predictions.\n",
    "3. Evaluate model performance using metrics like **RMSE** and **MAE**.\n",
    "4. Visualise recommendations and analyze patterns in user preferences and anime clusters.\n",
    "\n",
    "## Why This Matters\n",
    "Recommendation systems help users **discover content they are likely to enjoy** and are a critical part of many modern applications. By combining user behavior and content features, I aim to build a system that can provide **personalized anime recommendations** and gain insights into anime clustering and user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63b61d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822fba6",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Before building any recommendation system, we need to **clean, preprocess and structure the data**. This involves:\n",
    "\n",
    "1. **Handling missing values**. \n",
    "2. **Filtering invalid ratings**.  \n",
    "4. **Creating a user-item rating matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c340345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime dataset: (12294, 7)\n",
      "Ratings dataset: (7813737, 3)\n",
      "Unique anime: 12294\n",
      "Unique users: 73515\n",
      "Total ratings: rating\n",
      " 8     1646019\n",
      "-1     1476496\n",
      " 7     1375287\n",
      " 9     1254096\n",
      " 10     955715\n",
      " 6      637775\n",
      " 5      282806\n",
      " 4      104291\n",
      " 3       41453\n",
      " 2       23150\n",
      " 1       16649\n",
      "Name: count, dtype: int64\n",
      "   anime_id                              name  \\\n",
      "0     32281                    Kimi no Na wa.   \n",
      "1      5114  Fullmetal Alchemist: Brotherhood   \n",
      "2     28977                          Gintama°   \n",
      "3      9253                       Steins;Gate   \n",
      "4      9969                     Gintama&#039;   \n",
      "\n",
      "                                               genre   type episodes  rating  \\\n",
      "0               Drama, Romance, School, Supernatural  Movie        1    9.37   \n",
      "1  Action, Adventure, Drama, Fantasy, Magic, Mili...     TV       64    9.26   \n",
      "2  Action, Comedy, Historical, Parody, Samurai, S...     TV       51    9.25   \n",
      "3                                   Sci-Fi, Thriller     TV       24    9.17   \n",
      "4  Action, Comedy, Historical, Parody, Samurai, S...     TV       51    9.16   \n",
      "\n",
      "   members  \n",
      "0   200630  \n",
      "1   793665  \n",
      "2   114262  \n",
      "3   673572  \n",
      "4   151266  \n",
      "   user_id  anime_id  rating\n",
      "0        1        20      -1\n",
      "1        1        24      -1\n",
      "2        1        79      -1\n",
      "3        1       226      -1\n",
      "4        1       241      -1\n"
     ]
    }
   ],
   "source": [
    "# Loading datasets\n",
    "\n",
    "anime_df = pd.read_csv('data/anime.csv')\n",
    "ratings_df = pd.read_csv('data/rating.csv')\n",
    "\n",
    "print(f\"Anime dataset: {anime_df.shape}\")\n",
    "print(f\"Ratings dataset: {ratings_df.shape}\")\n",
    "\n",
    "# Displaying basic information\n",
    "\n",
    "print(f\"Unique anime: {anime_df['anime_id'].nunique()}\")\n",
    "print(f\"Unique users: {ratings_df['user_id'].nunique()}\")\n",
    "print(f\"Total ratings: {ratings_df['rating'].value_counts()}\")\n",
    "\n",
    "# Display data \n",
    "\n",
    "print(anime_df.head())\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af3f6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime entries after cleaning: 10931\n",
      "Ratings after cleaning: 6337241\n",
      "     user_id  anime_id  rating\n",
      "156        3        20       8\n",
      "157        3       154       6\n",
      "158        3       170       9\n",
      "159        3       199      10\n",
      "160        3       225       9\n",
      "Users: 60,970\n",
      "Items: 8,030\n",
      "Ratings: 6,314,650\n",
      "Sparsity: 98.71%\n",
      "Avg ratings per user: 103.6\n",
      "Avg ratings per item: 786.4\n"
     ]
    }
   ],
   "source": [
    "# Cleaning anime data\n",
    "\n",
    "anime_df['rating'] = pd.to_numeric(anime_df['rating'], errors='coerce')\n",
    "anime_df['episodes'] = pd.to_numeric(anime_df['episodes'], errors='coerce')\n",
    "anime_df['members'] = pd.to_numeric(anime_df['members'], errors='coerce')\n",
    "\n",
    "# removing all anime without ratings (-1) and adult content\n",
    "\n",
    "anime_clean = anime_df.dropna(subset=['rating'].copy())\n",
    "anime_clean = anime_clean[~anime_clean['genre'].str.contains('Hentai', case=False, na=False)].copy()\n",
    "print(f\"Anime entries after cleaning: {len(anime_clean)}\")\n",
    "\n",
    "# Cleaning rating data\n",
    "\n",
    "ratings_clean = ratings_df[(ratings_df['rating'] != -1) & (ratings_df['rating'].notna())].copy()\n",
    "print(f\"Ratings after cleaning: {len(ratings_clean)}\")\n",
    "\n",
    "# Removing all users and items with few interactions (deals with the sparcity)\n",
    "\n",
    "user_counts = ratings_clean['user_id'].value_counts()\n",
    "item_counts = ratings_clean['anime_id'].value_counts()\n",
    "\n",
    "# Keeping anime with at least 5 ratings\n",
    "\n",
    "min_user_ratings = 5\n",
    "min_item_ratings = 5\n",
    "\n",
    "valid_users = user_counts[user_counts >= min_user_ratings].index\n",
    "valid_items = item_counts[item_counts >= min_item_ratings].index\n",
    "\n",
    "ratings_filtered = ratings_clean[\n",
    "    (ratings_clean['user_id'].isin(valid_users)) & \n",
    "    (ratings_clean['anime_id'].isin(valid_items))\n",
    "].copy()\n",
    "\n",
    "print(ratings_filtered.head())\n",
    "\n",
    "# Calculating sparcity\n",
    "\n",
    "n_users = ratings_filtered['user_id'].nunique()\n",
    "n_items = ratings_filtered['anime_id'].nunique()\n",
    "n_ratings = len(ratings_filtered)\n",
    "sparsity = (1 - n_ratings / (n_users * n_items)) * 100\n",
    "\n",
    "print(f\"Users: {n_users:,}\")\n",
    "print(f\"Items: {n_items:,}\")\n",
    "print(f\"Ratings: {n_ratings:,}\")\n",
    "print(f\"Sparsity: {sparsity:.2f}%\")\n",
    "print(f\"Avg ratings per user: {n_ratings/n_users:.1f}\")\n",
    "print(f\"Avg ratings per item: {n_ratings/n_items:.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da8015",
   "metadata": {},
   "source": [
    "## Popularity Baseline Recommender\n",
    "\n",
    "The first step in building our recommendation system, we implement a **popularity-based baseline**. This baseline recommends the most popular items to all users, ignoring personal preferences. \n",
    "\n",
    "### How it Works:\n",
    "1. **Fit Phase**:\n",
    "- Compute average rating and count for each item.\n",
    "- Filter items with too few ratings (to reduce noise).\n",
    "- Rank items by average rating to determine popularity.\n",
    "2. **Prediction**:\n",
    "- If the item exists in the popular list, return its average rating.\n",
    "- Otherwise, return the global average rating.\n",
    "3. **Recommendation**:\n",
    "- Return top-N popular items.\n",
    "- Optionally, exclude items the user has already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "432ca8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityBaseline:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.popular_items = None\n",
    "        self.global_mean = None\n",
    "\n",
    "    def fit(self, ratings_df):\n",
    "        # Learns the most popular anime from the data\n",
    "\n",
    "        # Calculate the popularity score\n",
    "\n",
    "        item_stats = ratings_df.groupby('anime_id').agg({'rating' : ['mean','count']}).round(3)\n",
    "        item_stats.columns = ['avg_rating','rating_count']\n",
    "\n",
    "        # Only get anime with more than 10 ratings\n",
    "\n",
    "        item_stats = item_stats[item_stats['rating_count'] >= 10]\n",
    "\n",
    "        # Sort the anime by their avg_rating\n",
    "        self.popular_items = item_stats.sort_values('avg_rating', ascending = False)\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "\n",
    "        print(f\"Baseline trained on {len(item_stats)} popular items\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        # Predicts the rating for a user-item pair\n",
    "        if self.popular_items is None:\n",
    "            return self.global_mean\n",
    "        \n",
    "        if item_id in self.popular_items.index:\n",
    "            return self.popular_items.loc[item_id, 'avg_rating']\n",
    "        else:\n",
    "            return self.global_mean\n",
    "        \n",
    "    def recommend(self, user_id, n_recommendations, exclude_seen = None):\n",
    "        # Recomend the top popular items\n",
    "\n",
    "        recommendations = self.popular_items.head(n_recommendations).index.tolist()\n",
    "\n",
    "        if exclude_seen:\n",
    "            # Get all the popular items which have not been seen by the user\n",
    "            recommendations = [item for item in recommendations if item not in exclude_seen]\n",
    "\n",
    "            remaining = n_recommendations - len(recommendations)\n",
    "            if remaining > 0:\n",
    "                additional = self.popular_items.iloc[n_recommendations:n_recommendations+remaining*2]\n",
    "                for item in additional.index:\n",
    "                    if item not in exclude_seen:\n",
    "                        recommendations.append(item)\n",
    "                        if len(recommendations) >= n_recommendations:\n",
    "                            break\n",
    "\n",
    "            return recommendations[:n_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a0b509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Popularity Baseline\n",
      "Baseline trained on 7364 popular items\n",
      " 1. Gintama°                                 | Rating: 9.45 |Count: 1,182\n",
      " 1. Kimi no Na wa.                           | Rating: 9.42 |Count: 1,948\n",
      " 1. Ginga Eiyuu Densetsu                     | Rating: 9.39 |Count: 799\n",
      " 1. Fullmetal Alchemist: Brotherhood         | Rating: 9.32 |Count: 21,220\n",
      " 1. Gintama&#039;                            | Rating: 9.27 |Count: 3,098\n",
      " 1. Steins;Gate                              | Rating: 9.26 |Count: 17,019\n",
      " 1. Hunter x Hunter (2011)                   | Rating: 9.23 |Count: 7,418\n",
      " 1. Gintama                                  | Rating: 9.23 |Count: 4,222\n",
      " 1. Gintama&#039;: Enchousen                 | Rating: 9.20 |Count: 2,121\n",
      " 1. Gintama Movie: Kanketsu-hen - Yorozuya y | Rating: 9.19 |Count: 2,139\n"
     ]
    }
   ],
   "source": [
    "# Train baseline model\n",
    "print(\"Training Popularity Baseline\")\n",
    "baseline_model = PopularityBaseline()\n",
    "baseline_model.fit(ratings_filtered)\n",
    "\n",
    "# Show the top recommendation\n",
    "\n",
    "top_anime_ids = baseline_model.popular_items.head(10).index\n",
    "for i, anime_id in enumerate(top_anime_ids, 1):\n",
    "    anime_name = anime_clean[anime_clean['anime_id'] == anime_id]['name'].iloc[0]\n",
    "    avg_rating = baseline_model.popular_items.loc[anime_id, 'avg_rating']\n",
    "    rating_count = baseline_model.popular_items.loc[anime_id, 'rating_count']\n",
    "    print(f'{1:2d}. {anime_name[:40]:40} | Rating: {avg_rating:.2f} |Count: {rating_count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01b80f",
   "metadata": {},
   "source": [
    "## Content-Based Filtering for Anime Recommendations\n",
    "\n",
    "This section implements a **content-based recommendation system** that leverages **anime features** to generate personalised recommendations.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Feature Preparation**\n",
    "   - Genre features transformed using **TF-IDF**.\n",
    "   - Anime type features encoded as **one-hot vectors**.\n",
    "   - Numeric features like `rating`, `episodes`, and `members` **normalised**.\n",
    "   - Combined all features into a **single feature matrix**.\n",
    "\n",
    "2. **Similarity Computation**\n",
    "   - Compute **cosine similarity** between all anime.\n",
    "   - Store similarity matrix.\n",
    "\n",
    "3. **Personalized Recommendation**\n",
    "   - For each anime a user has rated, retrieve similar anime.\n",
    "   - Weight similarity scores by the user’s rating.\n",
    "   - Aggregate scores and sort to recommend the top-N anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0dfa3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedFilter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.item_features = None\n",
    "        self.item_similarity_matrix = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.anime_to_idx = None\n",
    "        self.idx_to_anime = None\n",
    "\n",
    "    def prepare_features(self, anime_df):\n",
    "        # preparing the feature matrix for anime\n",
    "\n",
    "\n",
    "        anime_ids = anime_df['anime_id'].values\n",
    "        self.anime_to_idx = {anime_id: idx for idx, anime_id in enumerate(anime_ids)}\n",
    "        self.idx_to_anime = {idx : anime_id for anime_id, idx in self.anime_to_idx.items()}\n",
    "\n",
    "        features_list = []\n",
    "\n",
    "        # Genres features using TF-IDF\n",
    "\n",
    "        genres = anime_df['genre'].fillna('Unknown')\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=50, stop_words=None)\n",
    "        genre_features = self.tfidf_vectorizer.fit_transform(genres).toarray()\n",
    "        features_list.append(genre_features)\n",
    "\n",
    "        # Type features\n",
    "\n",
    "        type_dummies = pd.get_dummies(anime_df['type']).values\n",
    "        features_list.append(type_dummies)\n",
    "\n",
    "        # Numerical features (normalised)\n",
    "\n",
    "        numerical_features = anime_df[['rating', 'episodes', 'members']].copy()\n",
    "        numerical_features['episodes'] = numerical_features['episodes'].fillna(1)\n",
    "\n",
    "        # normalise numerical features \n",
    "        for col in numerical_features.columns:\n",
    "            numerical_features[col] = (numerical_features[col] - numerical_features[col].mean()) / numerical_features[col].std()\n",
    "\n",
    "        features_list.append(numerical_features.values)\n",
    "\n",
    "        # Combine all features\n",
    "\n",
    "        self.item_features = np.hstack(features_list)\n",
    "\n",
    "        print(f\"Content features prepared: {self.item_features.shape}\")\n",
    "        return self\n",
    "    \n",
    "    def fit(self, anime_df):\n",
    "        # Train the content-based model\n",
    "\n",
    "        self.prepare_features(anime_df)\n",
    "\n",
    "        # Calculate the item similarity matrix\n",
    "\n",
    "        print(\"Calculating item simularity matrix\")\n",
    "        self.item_similarity_matrix = cosine_similarity(self.item_features)\n",
    "\n",
    "        print(\"Training complete\")\n",
    "        print(f\"Feature dimensions: {self.item_features.shape[1]}\")\n",
    "        print(f\"Items: {self.item_features.shape[0]}\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def get_silmilar_items(self, anime_id, n_similar=50):\n",
    "        # Get the most similar items to the anime given\n",
    "\n",
    "        if anime_id not in self.anime_to_idx:\n",
    "            return []\n",
    "        \n",
    "        item_idx = self.anime_to_idx[anime_id]\n",
    "        similarities = self.item_similarity_matrix[item_idx]\n",
    "\n",
    "        # Get indicies of the most similar items (except itself)\n",
    "\n",
    "        similar_indices = np.argsort(similarities)[::-1][1:n_similar+1]\n",
    "        similar_anime_ids = [self.idx_to_anime[idx] for idx in similar_indices]\n",
    "        similarity_scores = similarities[similar_indices]\n",
    "\n",
    "        return list(zip(similar_anime_ids, similarity_scores))\n",
    "    \n",
    "    def recommend(self, user_ratings, n_recommendations = 10):\n",
    "        # Recommend item based on the user's rating history\n",
    "        # user_ratings : dict {anime_id, rating}\n",
    "\n",
    "        if not user_ratings:\n",
    "            return []\n",
    "        \n",
    "        item_scores = defaultdict(float)\n",
    "        item_weights = defaultdict(float)\n",
    "\n",
    "        for rated_anime_id, rating in user_ratings.items():\n",
    "            if rated_anime_id not in self.anime_to_idx:\n",
    "                continue\n",
    "\n",
    "                # Get similar items to the given anime\n",
    "\n",
    "            similar_items = self.get_silmilar_items(rated_anime_id, n_similar=50)\n",
    "\n",
    "            for similar_anime_id, similarity in similar_items:\n",
    "                if similar_anime_id not in user_ratings: # Avoid recommending watched anime\n",
    "                    # Weight similarity by user's rating (higher rated items get more weight)\n",
    "                    weight = (rating - 5) * similarity \n",
    "                    item_scores[similar_anime_id] += item_weights\n",
    "                    item_weights[similar_anime_id] += abs(similarity)\n",
    "\n",
    "            # Calculate the final scores\n",
    "            final_scores = {}\n",
    "            for anime_id in item_scores:\n",
    "                if item_weights[anime_id] > 0:\n",
    "                       final_scores[anime_id] = item_scores[anime_id]/ item_weights[anime_id]\n",
    "\n",
    "            # Sort the final scores\n",
    "            recommendations = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            return [anime_id for anime_id, score in recommendations[:n_recommendations]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abaa2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training content-based filter\n",
      "Content features prepared: (10931, 56)\n",
      "Calculating item simularity matrix\n",
      "Training complete\n",
      "Feature dimensions: 56\n",
      "Items: 10931\n",
      "Finding similar anime to Tokyo Ghoul\n",
      "1. Elfen Lied                               | Similarity 0.999\n",
      "2. Tokyo Ghoul √A                           | Similarity 0.997\n",
      "3. Mirai Nikki (TV)                         | Similarity 0.996\n",
      "4. Death Note                               | Similarity 0.995\n",
      "5. Angel Beats!                             | Similarity 0.995\n"
     ]
    }
   ],
   "source": [
    "# Training content-based filter\n",
    "print(\"Training content-based filter\")\n",
    "content_filter = ContentBasedFilter()\n",
    "content_filter.fit(anime_clean)\n",
    "\n",
    "# Example: find an anime similar to a popular one\n",
    "sample_anime_id = anime_clean.iloc[449]['anime_id']\n",
    "sample_anime_name = anime_clean.iloc[449]['name']\n",
    "\n",
    "print(f\"Finding similar anime to {sample_anime_name}\")\n",
    "similar_anime = content_filter.get_silmilar_items(sample_anime_id, n_similar=5)\n",
    "\n",
    "for i, (anime_id, similarity) in enumerate(similar_anime, 1):\n",
    "    anime_name = anime_clean[anime_clean['anime_id'] == anime_id]['name']\n",
    "    if not anime_name.empty:\n",
    "        print(f\"{i}. {anime_name.iloc[0][:40]:40} | Similarity {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc31397",
   "metadata": {},
   "source": [
    "# User-Based Collaborative Filtering\n",
    "\n",
    "Now that we have popularity and content-based approaches, we implement **Collaborative Filtering (CF)**, \n",
    "which leverages similarities between users to generate recommendations. \n",
    "\n",
    "### Key Steps:\n",
    "1. **User-Item Matrix Creation**  \n",
    "   - Construct a sparse matrix where each row corresponds to a user and each column corresponds to an anime.  \n",
    "   - Matrix entries represent ratings (0 if unrated).  \n",
    "   - Also generate index mappings for users and items.  \n",
    "\n",
    "2. **User-Based CF Algorithm**  \n",
    "   - Calculate similarity between users using **cosine similarity** on their overlapping ratings.  \n",
    "   - Predict ratings for unseen anime based on ratings from similar users.  \n",
    "   - Use weighted averages where more similar users have greater influence.  \n",
    "\n",
    "3. **Model Training & Recommendations**  \n",
    "   - Fit the CF model on the user-item matrix.  \n",
    "   - For a given user, recommend anime they haven’t rated yet by predicting which ones they would likely rate highly.  \n",
    "   - Demonstrate recommendations for a sample user.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7d902b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(rating_df):\n",
    "    # Creates a spase user_item rating matrix\n",
    "\n",
    "    # Creat mappings for matrix indices\n",
    "\n",
    "    user_ids = sorted(ratings_df['user_id'].unique())\n",
    "    item_ids = sorted(ratings_df['anime_id'].unique())\n",
    "\n",
    "    user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    item_to_idx = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "\n",
    "    # Create matrix\n",
    "\n",
    "    n_users, n_items = len(user_ids), len(item_ids)\n",
    "    matrix = np.zeros((n_users, n_items))\n",
    "\n",
    "    for _, row in ratings_df.iterrows():\n",
    "        user_idx = user_to_idx[row['user_id']]\n",
    "        item_idx = item_to_idx[row['anime_id']]\n",
    "        matrix[user_idx, item_idx] = row['rating']\n",
    "\n",
    "    return matrix, user_to_idx, item_to_idx, user_ids, item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a86cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user-item matrix\n",
      "User-item matrix shape: (73515, 11200)\n",
      "Non-zero entries: 7,813,730\n",
      "Sparsity: 99.05%\n"
     ]
    }
   ],
   "source": [
    "# Creating sample matrix\n",
    "print('Creating user-item matrix')\n",
    "\n",
    "sample_size = 10000\n",
    "ratings_sample = ratings_filtered.sample(n=min(sample_size, len(ratings_filtered)), random_state=42)\n",
    "user_item_matrix, user_to_index, item_to_index, user_ids, item_ids = create_user_item_matrix(ratings_sample)\n",
    "\n",
    "print(f\"User-item matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(user_item_matrix):,}\")\n",
    "print(f\"Sparsity: {(1 - np.count_nonzero(user_item_matrix) / user_item_matrix.size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedCF:\n",
    "    \"\"\"\n",
    "    Highly optimized User-based Collaborative Filtering for large datasets\n",
    "    Designed to handle 70k+ users and 13k+ items efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=50, similarity_threshold=0.1, method='knn_sparse', \n",
    "                 knn_algorithm='auto', n_jobs=-1):\n",
    "        self.k = k\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.method = method  # 'knn_sparse', 'knn_sampling', 'locality_hashing'\n",
    "        self.knn_algorithm = knn_algorithm\n",
    "        self.n_jobs = n_jobs\n",
    "        \n",
    "        self.user_item_matrix = None\n",
    "        self.user_means = None\n",
    "        self.knn_model = None\n",
    "        self.sparse_matrix = None\n",
    "        \n",
    "    def fit(self, user_item_matrix):\n",
    "        \"\"\"Fit the model with optimizations for large datasets\"\"\"\n",
    "        print(f\"Training CF model on {user_item_matrix.shape} matrix...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Convert to sparse matrix for memory efficiency\n",
    "        if not sp.issparse(user_item_matrix):\n",
    "            print(\"Converting to sparse matrix...\")\n",
    "            self.sparse_matrix = csr_matrix(user_item_matrix)\n",
    "            # Keep a small dense version for quick access during prediction\n",
    "            self.user_item_matrix = user_item_matrix\n",
    "        else:\n",
    "            self.sparse_matrix = user_item_matrix\n",
    "            self.user_item_matrix = user_item_matrix.toarray()\n",
    "        \n",
    "        print(f\"Sparsity: {1 - self.sparse_matrix.nnz / (self.sparse_matrix.shape[0] * self.sparse_matrix.shape[1]):.4f}\")\n",
    "        \n",
    "        # Calculate user means efficiently\n",
    "        self._calculate_user_means()\n",
    "        \n",
    "        # Fit the appropriate model\n",
    "        if self.method == 'knn_sparse':\n",
    "            self._fit_sparse_knn()\n",
    "        elif self.method == 'knn_sampling':\n",
    "            self._fit_sampled_knn()\n",
    "        elif self.method == 'locality_hashing':\n",
    "            self._fit_lsh()\n",
    "        \n",
    "        fit_time = time.time() - start_time\n",
    "        print(f\"Model trained in {fit_time:.2f} seconds\")\n",
    "        \n",
    "    def _calculate_user_means(self):\n",
    "        \"\"\"Calculate user means efficiently for sparse matrix\"\"\"\n",
    "        if sp.issparse(self.sparse_matrix):\n",
    "            # For sparse matrix, calculate means only on non-zero entries\n",
    "            self.user_means = np.array([\n",
    "                np.mean(self.sparse_matrix.getrow(i).data) if self.sparse_matrix.getrow(i).nnz > 0 else 0\n",
    "                for i in range(self.sparse_matrix.shape[0])\n",
    "            ])\n",
    "        else:\n",
    "            self.user_means = np.array([\n",
    "                user_ratings[user_ratings > 0].mean() if np.any(user_ratings > 0) else 0 \n",
    "                for user_ratings in self.user_item_matrix\n",
    "            ])\n",
    "    \n",
    "    def _fit_sparse_knn(self):\n",
    "        \"\"\"Fit KNN model using sparse matrix operations\"\"\"\n",
    "        print(\"Fitting sparse KNN model...\")\n",
    "        \n",
    "        # Use brute force with cosine similarity for sparse data\n",
    "        self.knn_model = NearestNeighbors(\n",
    "            n_neighbors=min(self.k + 1, self.sparse_matrix.shape[0]),\n",
    "            algorithm='brute',  # Best for sparse, high-dimensional data\n",
    "            metric='cosine',\n",
    "            n_jobs=self.n_jobs\n",
    "        )\n",
    "        \n",
    "        # Fit on sparse matrix directly\n",
    "        self.knn_model.fit(self.sparse_matrix)\n",
    "    \n",
    "    def _fit_sampled_knn(self):\n",
    "        \"\"\"Fit KNN using random sampling to reduce computation\"\"\"\n",
    "        print(\"Fitting sampled KNN model...\")\n",
    "        \n",
    "        n_users = self.sparse_matrix.shape[0]\n",
    "        # Sample a subset of users for building the index\n",
    "        sample_size = min(10000, n_users // 2)  # Use at most 10k users for index\n",
    "        \n",
    "        sample_indices = np.random.choice(n_users, size=sample_size, replace=False)\n",
    "        sample_matrix = self.sparse_matrix[sample_indices]\n",
    "        \n",
    "        self.knn_model = NearestNeighbors(\n",
    "            n_neighbors=min(self.k + 1, sample_size),\n",
    "            algorithm=self.knn_algorithm,\n",
    "            metric='cosine',\n",
    "            n_jobs=self.n_jobs\n",
    "        )\n",
    "        \n",
    "        self.knn_model.fit(sample_matrix)\n",
    "        self.sample_indices = sample_indices\n",
    "    \n",
    "    def _fit_lsh(self):\n",
    "        \"\"\"Fit using Locality Sensitive Hashing for very large datasets\"\"\"\n",
    "        # This is a placeholder for LSH implementation\n",
    "        # You can use libraries like datasketch or implement MinHash LSH\n",
    "        print(\"LSH not implemented yet, falling back to sparse KNN\")\n",
    "        self._fit_sparse_knn()\n",
    "    \n",
    "    def _find_similar_users_fast(self, user_idx):\n",
    "        \"\"\"Find similar users using the fitted model\"\"\"\n",
    "        if self.method == 'knn_sparse':\n",
    "            user_vector = self.sparse_matrix.getrow(user_idx)\n",
    "            distances, indices = self.knn_model.kneighbors(user_vector)\n",
    "            \n",
    "            # Convert distances to similarities and remove self\n",
    "            similarities = 1 - distances[0][1:]\n",
    "            neighbor_indices = indices[0][1:]\n",
    "            \n",
    "        elif self.method == 'knn_sampling':\n",
    "            user_vector = self.sparse_matrix.getrow(user_idx)\n",
    "            distances, indices = self.knn_model.kneighbors(user_vector)\n",
    "            \n",
    "            # Map back to original indices\n",
    "            similarities = 1 - distances[0]\n",
    "            neighbor_indices = self.sample_indices[indices[0]]\n",
    "            \n",
    "            # Remove self if present\n",
    "            mask = neighbor_indices != user_idx\n",
    "            similarities = similarities[mask]\n",
    "            neighbor_indices = neighbor_indices[mask]\n",
    "        \n",
    "        return neighbor_indices, similarities\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        \"\"\"Fast prediction using optimized similarity search\"\"\"\n",
    "        # Find similar users efficiently\n",
    "        similar_users, similarities = self._find_similar_users_fast(user_idx)\n",
    "        \n",
    "        # Filter users who have rated this item\n",
    "        valid_similarities = []\n",
    "        valid_ratings = []\n",
    "        \n",
    "        for i, other_user_idx in enumerate(similar_users[:self.k]):  # Limit to top-k\n",
    "            if self.user_item_matrix[other_user_idx, item_idx] > 0:\n",
    "                sim = similarities[i]\n",
    "                if sim > self.similarity_threshold:\n",
    "                    valid_similarities.append(sim)\n",
    "                    valid_ratings.append(self.user_item_matrix[other_user_idx, item_idx])\n",
    "        \n",
    "        # If no similar users found, return user mean or global default\n",
    "        if not valid_similarities:\n",
    "            return self.user_means[user_idx] if self.user_means[user_idx] > 0 else 3.0\n",
    "        \n",
    "        # Weighted average prediction\n",
    "        valid_similarities = np.array(valid_similarities)\n",
    "        valid_ratings = np.array(valid_ratings)\n",
    "        \n",
    "        predicted_rating = np.sum(valid_similarities * valid_ratings) / np.sum(valid_similarities)\n",
    "        return np.clip(predicted_rating, 1, 10)\n",
    "    \n",
    "    def predict_batch(self, user_item_pairs):\n",
    "        \"\"\"Predict ratings for multiple user-item pairs efficiently\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Group by user to reuse similarity computations\n",
    "        user_groups = {}\n",
    "        for i, (user_idx, item_idx) in enumerate(user_item_pairs):\n",
    "            if user_idx not in user_groups:\n",
    "                user_groups[user_idx] = []\n",
    "            user_groups[user_idx].append((i, item_idx))\n",
    "        \n",
    "        results = [0] * len(user_item_pairs)\n",
    "        \n",
    "        for user_idx, items in user_groups.items():\n",
    "            # Find similar users once per user\n",
    "            similar_users, similarities = self._find_similar_users_fast(user_idx)\n",
    "            \n",
    "            for original_idx, item_idx in items:\n",
    "                # Use precomputed similar users for this prediction\n",
    "                valid_similarities = []\n",
    "                valid_ratings = []\n",
    "                \n",
    "                for i, other_user_idx in enumerate(similar_users[:self.k]):\n",
    "                    if self.user_item_matrix[other_user_idx, item_idx] > 0:\n",
    "                        sim = similarities[i]\n",
    "                        if sim > self.similarity_threshold:\n",
    "                            valid_similarities.append(sim)\n",
    "                            valid_ratings.append(self.user_item_matrix[other_user_idx, item_idx])\n",
    "                \n",
    "                if not valid_similarities:\n",
    "                    pred = self.user_means[user_idx] if self.user_means[user_idx] > 0 else 3.0\n",
    "                else:\n",
    "                    valid_similarities = np.array(valid_similarities)\n",
    "                    valid_ratings = np.array(valid_ratings)\n",
    "                    pred = np.sum(valid_similarities * valid_ratings) / np.sum(valid_similarities)\n",
    "                    pred = np.clip(pred, 1, 10)\n",
    "                \n",
    "                results[original_idx] = pred\n",
    "        \n",
    "        return np.array(results)\n",
    "    \n",
    "    def recommend(self, user_idx, n_recommendations=10, exclude_seen=True):\n",
    "        \"\"\"Generate recommendations efficiently\"\"\"\n",
    "        user_ratings = self.user_item_matrix[user_idx]\n",
    "        \n",
    "        if exclude_seen:\n",
    "            candidate_items = np.where(user_ratings == 0)[0]\n",
    "        else:\n",
    "            candidate_items = np.arange(len(user_ratings))\n",
    "        \n",
    "        if len(candidate_items) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Limit candidates if too many (for performance)\n",
    "        if len(candidate_items) > 1000:\n",
    "            # Sample candidates or use other heuristics\n",
    "            candidate_items = np.random.choice(candidate_items, size=1000, replace=False)\n",
    "        \n",
    "        # Batch predict for efficiency\n",
    "        user_item_pairs = [(user_idx, item_idx) for item_idx in candidate_items]\n",
    "        predictions = self.predict_batch(user_item_pairs)\n",
    "        \n",
    "        # Sort and return top N\n",
    "        item_pred_pairs = list(zip(candidate_items, predictions))\n",
    "        item_pred_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return [item_idx for item_idx, _ in item_pred_pairs[:n_recommendations]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b395c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training user-based collaborative filtering\n",
      "Training CF model on (73515, 11200) matrix...\n",
      "Converting to sparse matrix...\n",
      "Sparsity: 0.9905\n",
      "Fitting sparse KNN model...\n",
      "Model trained in 6.64 seconds\n",
      "Sample recommendations for User : 1:\n",
      "1. Great Teacher Onizuka                    | Predicted Rating: 10.00\n",
      "2. Fullmetal Alchemist                      | Predicted Rating: 10.00\n",
      "3. Code Geass: Boukoku no Akito 1 - Yokuryu | Predicted Rating: 9.00\n",
      "4. Steins;Gate                              | Predicted Rating: 9.00\n",
      "5. Sword Art Online: Extra Edition          | Predicted Rating: 8.00\n"
     ]
    }
   ],
   "source": [
    "# Training user-based CF\n",
    "print(\"Training user-based collaborative filtering\")\n",
    "user_cf = UserBasedCF(k=50, method='knn_sparse', n_jobs=-1)\n",
    "user_cf.fit(user_item_matrix)\n",
    "\n",
    "# Example recommendation\n",
    "\n",
    "test_user_idx = 0\n",
    "if test_user_idx < len(user_item_matrix):\n",
    "    print(f\"Sample recommendations for User : {user_ids[test_user_idx]}:\")\n",
    "    recommendations = user_cf.recommend(test_user_idx, n_recommendations=5)\n",
    "\n",
    "    for i, item_idx in enumerate(recommendations,1):\n",
    "        item_id = item_ids[item_idx]\n",
    "        anime_name = anime_clean[anime_clean[\"anime_id\"] == item_id]['name']\n",
    "        if not anime_name.empty:\n",
    "            predicted_rating = user_cf.predict(test_user_idx, item_idx)\n",
    "            print(f\"{i}. {anime_name.iloc[0][:40]:40} | Predicted Rating: {predicted_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b65033",
   "metadata": {},
   "source": [
    "## Hybrid Recommendation System – Matrix Factorisation (SVD)\n",
    "\n",
    "Matrix Factorisation is a latent-factor approach that reduces the high-dimensional user–item rating matrix into a smaller set of **hidden features** that capture patterns in user preferences and item attributes.\n",
    "\n",
    "### Key Idea\n",
    "Instead of relying only on observed similarities (like CF) or metadata (like content-based), this method learns a **compressed representation** of both users and items:\n",
    "- **Users** are mapped into a latent feature space based on their rating behavior.\n",
    "- **Items** are mapped into the same latent space based on who rated them and how.\n",
    "- The interaction between a user’s vector and an item’s vector predicts ratings.\n",
    "\n",
    "### Steps in Implementation\n",
    "1. **Global Mean Centering**  \n",
    "   - Calculate the average rating across the dataset.  \n",
    "   - Subtract this mean from observed ratings to focus on relative preferences.\n",
    "\n",
    "2. **Truncated SVD**  \n",
    "   - Decompose the user–item matrix into:  \n",
    "     - `user_factors` → low-dimensional user representations.  \n",
    "     - `item_factors` → low-dimensional item representations.  \n",
    "\n",
    "3. **Prediction**  \n",
    "   - For user *u* and item *i*, the rating is estimated as:  \n",
    "     \\[\n",
    "     \\hat{r}_{ui} = \\mu + U_u \\cdot V_i\n",
    "     \\]  \n",
    "     Where \\(\\mu\\) is the global mean rating, and \\(U_u, V_i\\) are latent vectors.\n",
    "\n",
    "4. **Recommendation**  \n",
    "   - Predict scores for all unrated items.  \n",
    "   - Rank them and recommend the top-N highest scoring items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08a1db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorisationSVD:\n",
    "    # Matrix Factorisation using Truncated SVD\n",
    "\n",
    "    def __init__(self, n_components = 50):\n",
    "        self.n_components = n_components\n",
    "        self.svd = None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.global_mean = None\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        # Training the SVD model \n",
    "\n",
    "        print(f\"Training matrix factorisation (SVD) with {self.n_components} components\")\n",
    "\n",
    "        self.global_mean = user_item_matrix[user_item_matrix > 0].mean()\n",
    "\n",
    "        # Center centered matrix (only for non-zero entries)\n",
    "\n",
    "        centered_matrix = user_item_matrix.copy()\n",
    "        mask = user_item_matrix > 0\n",
    "        centered_matrix[mask] -= self.global_mean\n",
    "\n",
    "        # Apply SVD\n",
    "\n",
    "        self.svd = TruncatedSVD(n_components=self.n_components, random_state=42)\n",
    "        self.user_factors = self.svd.fit_transform(centered_matrix)\n",
    "\n",
    "        # Get item factors\n",
    "\n",
    "        self.item_factors = self.svd.components_.T\n",
    "\n",
    "        print(f\"SVD model trained\")\n",
    "        print(f\"Explained variance ratio: {self.svd.explained_variance_ratio_.sum():.3f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        # Predict the rating for user-item pairs\n",
    "\n",
    "        prediction = self.global_mean + np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "        return np.clip(prediction, 1, 10)\n",
    "    \n",
    "    def recommend(self, user_idx, n_recommendations=10, exclude_seen=True):\n",
    "        # Recommend items for a user\n",
    "        # Calculate predictions for all items\n",
    "\n",
    "        user_vector = self.user_factors[user_idx]\n",
    "        predictions = self.global_mean + np.dot(self.item_factors, user_vector)\n",
    "\n",
    "        # Gret indices of top predictions\n",
    "\n",
    "        top_indices = np.argsort(predictions)[::-1]\n",
    "\n",
    "        recommendations = []\n",
    "\n",
    "        for item_idx in top_indices:\n",
    "            if len(recommendations) >= n_recommendations:\n",
    "                break\n",
    "\n",
    "            # Exclude items the user has already seen\n",
    "            if exclude_seen and user_item_matrix[user_idx, item_idx] > 0:\n",
    "                continue\n",
    "\n",
    "            recommendations.append(item_idx)\n",
    "\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2256178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorisation Mode\n",
      "Training matrix factorisation (SVD) with 50 components\n",
      "SVD model trained\n",
      "Explained variance ratio: 0.243\n",
      "SVD Recommendation for User 1:\n",
      "1. Highschool of the Dead: Drifters of the  | Predicted Rating: 8.15\n",
      "2. Mahou Shoujo Madoka★Magica               | Predicted Rating: 8.13\n",
      "3. K-On!                                    | Predicted Rating: 8.10\n",
      "4. Hellsing                                 | Predicted Rating: 8.09\n",
      "5. Bishoujo Senshi Sailor Moon              | Predicted Rating: 8.06\n"
     ]
    }
   ],
   "source": [
    "# Training SVD model \n",
    "\n",
    "print(\"Training Matrix Factorisation Mode\")\n",
    "svd_model = MatrixFactorisationSVD(n_components=50)\n",
    "svd_model.fit(user_item_matrix)\n",
    "\n",
    "# Example recommendations\n",
    "\n",
    "if test_user_idx < len(user_item_matrix):\n",
    "    print(f\"SVD Recommendation for User {user_ids[test_user_idx]}:\")\n",
    "    svd_recommendations = svd_model.recommend(test_user_idx, n_recommendations = 5)\n",
    "    for i, item_idx in enumerate(svd_recommendations, 1):\n",
    "        item_id = item_ids[item_idx]\n",
    "        anime_name = anime_clean[anime_clean['anime_id'] == item_id]['name']\n",
    "        if not anime_name.empty:\n",
    "            predicted_rating = svd_model.predict(test_user_idx, item_idx)\n",
    "            print(f\"{i}. {anime_name.iloc[0][:40]:40} | Predicted Rating: {predicted_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86023b1",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To assess the performance of our recommendation models, we split the dataset into **train** and **test sets**.  \n",
    "We use a **temporal split**, ensuring that for each user, their most recent ratings form the test set (20% of their ratings).  \n",
    "\n",
    "We evaluate the models using two types of metrics:\n",
    "\n",
    "- **Rating Prediction Metrics**\n",
    "  - **RMSE (Root Mean Squared Error):** Measures the average difference between predicted and actual ratings. Lower is better.\n",
    "  - **MAE (Mean Absolute Error):** Measures the absolute average difference. Lower is better.\n",
    "\n",
    "- **Recommendation Quality Metrics**\n",
    "  - **Precision@K:** Proportion of recommended items that are relevant.\n",
    "  - **Recall@K:** Proportion of relevant items that were recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dd39cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_temporal(ratings_df, test_ratio = 0.2):\n",
    "    # Split the data by time (the last 20% of each users ratings as test)\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    for user_id in ratings_df['user_id'].unique():\n",
    "        user_ratings = ratings_df[ratings_df['user_id'] == user_id].sort_values('anime_id')\n",
    "        n_ratings = len(user_ratings)\n",
    "        n_test = max(1, int(n_ratings * test_ratio))\n",
    "\n",
    "        train_data.append(user_ratings.iloc[:-n_test])\n",
    "        test_data.append(user_ratings.iloc[-n_test:])\n",
    "\n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    test_df = pd.concat(test_data, ignore_index=True)\n",
    "    \n",
    "    return train_df, test_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "090c1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"Calculate RMSE and MAE\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4c6f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(model, test_data, user_to_index, item_to_index, k = 10):\n",
    "    # Evaluate the quality of the recommendations\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for user_id in test_data['user_id'].unique():\n",
    "        if user_id not in user_to_index:\n",
    "            continue\n",
    "\n",
    "        user_idx = user_to_index[user_id]\n",
    "        user_test_items = set(test_data[test_data['user_id'] == user_id]['anime_id'].values)\n",
    "        \n",
    "        try:\n",
    "            recommendations = model.recommend(user_idx, n_recommendations = k)\n",
    "            if hasattr(model, 'recommend') and recommendations:\n",
    "                # Convert the indices to item IDs if needed\n",
    "\n",
    "                if isinstance(recommendations[0], int) and recommendations[0] < len(item_ids):\n",
    "                    recommended_items = set(item_to_index[idx] for idx in recommendations)\n",
    "                else:\n",
    "                    recommended_items = set(recommendations)\n",
    "\n",
    "                relevent_items = user_test_items.intersection(recommendations)\n",
    "\n",
    "                precision = len(relevent_items) / len(recommended_items) if recommended_items else 0\n",
    "                recall = len(relevent_items) / len(user_test_items) if user_test_items else 0\n",
    "\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "        \n",
    "        except:\n",
    "            print(\"failed\")\n",
    "            continue\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    \n",
    "    return avg_precision, avg_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd51d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/test split\n",
      "Train set: 1,811 ratings\n",
      "Test set: 8,189 ratings\n",
      "\n",
      "Retraining models on training data...\n",
      "Baseline trained on 3 popular items\n",
      "Training CF model on (73515, 11200) matrix...\n",
      "Converting to sparse matrix...\n",
      "Sparsity: 0.9905\n",
      "Fitting sparse KNN model...\n",
      "Model trained in 6.91 seconds\n",
      "Training matrix factorisation (SVD) with 30 components\n",
      "SVD model trained\n",
      "Explained variance ratio: 0.199\n",
      "Evaluation rating prediction accuracy\n"
     ]
    }
   ],
   "source": [
    "# Create train / test split\n",
    "\n",
    "print(\"Creating train/test split\")\n",
    "train_ratings, test_ratings = train_test_split_temporal(ratings_sample, test_ratio=0.2)\n",
    "\n",
    "print(f\"Train set: {len(train_ratings):,} ratings\")\n",
    "print(f\"Test set: {len(test_ratings):,} ratings\")\n",
    "\n",
    "# Retrain models on training data\n",
    "\n",
    "print(\"\\nRetraining models on training data...\")\n",
    "\n",
    "# Retrain baseline\n",
    "\n",
    "baseline_eval = PopularityBaseline()\n",
    "baseline_eval.fit(train_ratings)\n",
    "\n",
    "# Retrain collaborative filtering\n",
    "\n",
    "train_matrix, train_user_to_idx, train_item_to_idx, train_user_ids, train_item_ids = create_user_item_matrix(train_ratings)\n",
    "user_cf_eval = UserBasedCF(k=50, method='knn_sparse', n_jobs=-1)\n",
    "user_cf_eval.fit(train_matrix)\n",
    "\n",
    "# Retrain SVD\n",
    "\n",
    "svd_eval = MatrixFactorisationSVD(n_components=30)\n",
    "svd_eval.fit(train_matrix)\n",
    "\n",
    "# Evaluate rating prediction accuracy\n",
    "\n",
    "print(\"Evaluation rating prediction accuracy\")\n",
    "\n",
    "# Collect predictions for test set\n",
    "\n",
    "baseline_predictions = []\n",
    "user_cf_predictions = []\n",
    "svd_predictions = []\n",
    "true_ratings = []\n",
    "\n",
    "# Sample some predictions for evaluation\n",
    "\n",
    "test_sample = test_ratings.sample(min(1000, len(test_ratings)), random_state=42)\n",
    "\n",
    "for _, row in test_sample.iterrows():\n",
    "    user_id, item_id, true_rating = row['user_id'], row['anime_id'], row['rating']\n",
    "\n",
    "    # Skip if the user or the item is not in the training data\n",
    "\n",
    "    if user_id not in train_user_to_idx or item_id not in train_item_to_idx:\n",
    "        continue\n",
    "\n",
    "    user_idx = train_user_to_idx[user_id]\n",
    "    item_idx = train_item_to_idx[item_id]\n",
    "    \n",
    "    # Get predictions\n",
    "\n",
    "    baseline_pred = baseline_eval.predict(user_id, item_id)\n",
    "    user_cf_pred = user_cf_eval.predict(user_idx, item_idx)\n",
    "    svd_pred = svd_eval.predict(user_idx, item_idx)\n",
    "    \n",
    "    baseline_predictions.append(baseline_pred)\n",
    "    user_cf_predictions.append(user_cf_pred)\n",
    "    svd_predictions.append(svd_pred)\n",
    "    true_ratings.append(true_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c84eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PREDICTION ACCURACY RESULTS:\n",
      "Model                RMSE     MAE     \n",
      "--------------------------------------------------\n",
      "Popularity Baseline  1.596    1.284   \n",
      "User-based CF        1.534    1.155   \n",
      "Matrix Factorization 1.375    1.050   \n"
     ]
    }
   ],
   "source": [
    "if true_ratings:\n",
    "    baseline_rmse, baseline_mae = evaluate_predictions(true_ratings, baseline_predictions)\n",
    "    user_cf_rmse, user_cf_mae = evaluate_predictions(true_ratings, user_cf_predictions)\n",
    "    svd_rmse, svd_mae = evaluate_predictions(true_ratings, svd_predictions)\n",
    "    \n",
    "    print(f\"\\nPREDICTION ACCURACY RESULTS:\")\n",
    "    print(f\"{'Model':<20} {'RMSE':<8} {'MAE':<8}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Popularity Baseline':<20} {baseline_rmse:<8.3f} {baseline_mae:<8.3f}\")\n",
    "    print(f\"{'User-based CF':<20} {user_cf_rmse:<8.3f} {user_cf_mae:<8.3f}\")\n",
    "    print(f\"{'Matrix Factorization':<20} {svd_rmse:<8.3f} {svd_mae:<8.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbd336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline model\n",
      "Evaluating user CF model\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model\n",
    "\n",
    "print(\"Evaluating baseline model\")\n",
    "baseline_precision, baseline_recall = evaluate_recommendations(baseline_eval, test_ratings, train_user_to_idx, train_item_to_idx)\n",
    "print(\"Evaluating user CF model\")\n",
    "user_cf_precision, user_cf_recall = evaluate_recommendations(user_cf_eval, test_ratings, train_user_to_idx, train_item_to_idx)\n",
    "print(\"Evaluating Hybrid model\")\n",
    "svd_precision, svd_recall = evaluate_recommendations(svd_eval, test_ratings, train_user_to_idx, train_item_to_idx)\n",
    "\n",
    "print(f\"\\nRECOMMENDATION QUALITY RESULTS:\")\n",
    "print(f\"{'Model':<20} {'Precision@10':<12} {'Recall@10':<12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Popularity Baseline':<20} {baseline_precision:<12.3f} {baseline_recall:<12.3f}\")\n",
    "print(f\"{'User-based CF':<20} {user_cf_precision:<12.3f} {user_cf_recall:<12.3f}\")\n",
    "print(f\"{'Matrix Factorization':<20} {svd_precision:<12.3f} {svd_recall:<12.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
