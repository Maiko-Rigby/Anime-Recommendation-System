{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cbd7356",
   "metadata": {},
   "source": [
    "# Anime Recommendation System: Introduction\n",
    "\n",
    "## Objective\n",
    "The goal of this project is to build a **recommendation system** that can suggest anime to users based on their past ratings and anime characteristics.\n",
    "\n",
    "## Dataset Overview\n",
    "We are using the [Anime Dataset](#https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database?select=rating.csv) containing:\n",
    "- **Anime information**: anime_id,name,genre,type,episodes,rating,members.\n",
    "- **User ratings**: user_id,anime_id,rating.\n",
    "\n",
    "## Project Goals\n",
    "1. Explore and preprocess the dataset to handle sparsity, missing values and noisy ratings.\n",
    "2. Implement multiple recommendation techniques:\n",
    "    - **Baseline recommendation**: Simple popularity-based recommendations.\n",
    "    - **Collaborative Filtering (CF)**: Suggest anime based on user-user or item-item similarity.\n",
    "    - **Content-Based Filtering (CBF)**: Recommend anime using genre, type, and other features.\n",
    "    - **Hybrid approaches**: Combine CF and CBF for better predictions.\n",
    "3. Evaluate model performance using metrics like **RMSE** and **MAE**.\n",
    "4. Visualise recommendations and analyze patterns in user preferences and anime clusters.\n",
    "\n",
    "## Why This Matters\n",
    "Recommendation systems help users **discover content they are likely to enjoy** and are a critical part of many modern applications. By combining user behavior and content features, I aim to build a system that can provide **personalized anime recommendations** and gain insights into anime clustering and user preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b61d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a822fba6",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Before building any recommendation system, we need to **clean, preprocess and structure the data**. This involves:\n",
    "\n",
    "1. **Handling missing values**. \n",
    "2. **Filtering invalid ratings**.  \n",
    "4. **Creating a user-item rating matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c340345e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime dataset: (12294, 7)\n",
      "Ratings dataset: (7813737, 3)\n",
      "Unique anime: 12294\n",
      "Unique users: 73515\n",
      "Total ratings: rating\n",
      " 8     1646019\n",
      "-1     1476496\n",
      " 7     1375287\n",
      " 9     1254096\n",
      " 10     955715\n",
      " 6      637775\n",
      " 5      282806\n",
      " 4      104291\n",
      " 3       41453\n",
      " 2       23150\n",
      " 1       16649\n",
      "Name: count, dtype: int64\n",
      "   anime_id                              name  \\\n",
      "0     32281                    Kimi no Na wa.   \n",
      "1      5114  Fullmetal Alchemist: Brotherhood   \n",
      "2     28977                          GintamaÂ°   \n",
      "3      9253                       Steins;Gate   \n",
      "4      9969                     Gintama&#039;   \n",
      "\n",
      "                                               genre   type episodes  rating  \\\n",
      "0               Drama, Romance, School, Supernatural  Movie        1    9.37   \n",
      "1  Action, Adventure, Drama, Fantasy, Magic, Mili...     TV       64    9.26   \n",
      "2  Action, Comedy, Historical, Parody, Samurai, S...     TV       51    9.25   \n",
      "3                                   Sci-Fi, Thriller     TV       24    9.17   \n",
      "4  Action, Comedy, Historical, Parody, Samurai, S...     TV       51    9.16   \n",
      "\n",
      "   members  \n",
      "0   200630  \n",
      "1   793665  \n",
      "2   114262  \n",
      "3   673572  \n",
      "4   151266  \n",
      "   user_id  anime_id  rating\n",
      "0        1        20      -1\n",
      "1        1        24      -1\n",
      "2        1        79      -1\n",
      "3        1       226      -1\n",
      "4        1       241      -1\n"
     ]
    }
   ],
   "source": [
    "# Loading datasets\n",
    "\n",
    "anime_df = pd.read_csv('data/anime.csv')\n",
    "ratings_df = pd.read_csv('data/rating.csv')\n",
    "\n",
    "print(f\"Anime dataset: {anime_df.shape}\")\n",
    "print(f\"Ratings dataset: {ratings_df.shape}\")\n",
    "\n",
    "# Displaying basic information\n",
    "\n",
    "print(f\"Unique anime: {anime_df['anime_id'].nunique()}\")\n",
    "print(f\"Unique users: {ratings_df['user_id'].nunique()}\")\n",
    "print(f\"Total ratings: {ratings_df['rating'].value_counts()}\")\n",
    "\n",
    "# Display data \n",
    "\n",
    "print(anime_df.head())\n",
    "print(ratings_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af3f6a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime entries after cleaning: 10931\n",
      "Ratings after cleaning: 6337241\n",
      "     user_id  anime_id  rating\n",
      "156        3        20       8\n",
      "157        3       154       6\n",
      "158        3       170       9\n",
      "159        3       199      10\n",
      "160        3       225       9\n",
      "Users: 60,970\n",
      "Items: 8,030\n",
      "Ratings: 6,314,650\n",
      "Sparsity: 98.71%\n",
      "Avg ratings per user: 103.6\n",
      "Avg ratings per item: 786.4\n"
     ]
    }
   ],
   "source": [
    "# Cleaning anime data\n",
    "\n",
    "anime_df['rating'] = pd.to_numeric(anime_df['rating'], errors='coerce')\n",
    "anime_df['episodes'] = pd.to_numeric(anime_df['episodes'], errors='coerce')\n",
    "anime_df['members'] = pd.to_numeric(anime_df['members'], errors='coerce')\n",
    "\n",
    "# removing all anime without ratings (-1) and adult content\n",
    "\n",
    "anime_clean = anime_df.dropna(subset=['rating'].copy())\n",
    "anime_clean = anime_clean[~anime_clean['genre'].str.contains('Hentai', case=False, na=False)].copy()\n",
    "print(f\"Anime entries after cleaning: {len(anime_clean)}\")\n",
    "\n",
    "# Cleaning rating data\n",
    "\n",
    "ratings_clean = ratings_df[(ratings_df['rating'] != -1) & (ratings_df['rating'].notna())].copy()\n",
    "print(f\"Ratings after cleaning: {len(ratings_clean)}\")\n",
    "\n",
    "# Removing all users and items with few interactions (deals with the sparcity)\n",
    "\n",
    "user_counts = ratings_clean['user_id'].value_counts()\n",
    "item_counts = ratings_clean['anime_id'].value_counts()\n",
    "\n",
    "# Keeping anime with at least 5 ratings\n",
    "\n",
    "min_user_ratings = 5\n",
    "min_item_ratings = 5\n",
    "\n",
    "valid_users = user_counts[user_counts >= min_user_ratings].index\n",
    "valid_items = item_counts[item_counts >= min_item_ratings].index\n",
    "\n",
    "ratings_filtered = ratings_clean[\n",
    "    (ratings_clean['user_id'].isin(valid_users)) & \n",
    "    (ratings_clean['anime_id'].isin(valid_items))\n",
    "].copy()\n",
    "\n",
    "print(ratings_filtered.head())\n",
    "\n",
    "# Calculating sparcity\n",
    "\n",
    "n_users = ratings_filtered['user_id'].nunique()\n",
    "n_items = ratings_filtered['anime_id'].nunique()\n",
    "n_ratings = len(ratings_filtered)\n",
    "sparsity = (1 - n_ratings / (n_users * n_items)) * 100\n",
    "\n",
    "print(f\"Users: {n_users:,}\")\n",
    "print(f\"Items: {n_items:,}\")\n",
    "print(f\"Ratings: {n_ratings:,}\")\n",
    "print(f\"Sparsity: {sparsity:.2f}%\")\n",
    "print(f\"Avg ratings per user: {n_ratings/n_users:.1f}\")\n",
    "print(f\"Avg ratings per item: {n_ratings/n_items:.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da8015",
   "metadata": {},
   "source": [
    "## Popularity Baseline Recommender\n",
    "\n",
    "The first step in building our recommendation system, we implement a **popularity-based baseline**. This baseline recommends the most popular items to all users, ignoring personal preferences. \n",
    "\n",
    "### How it Works:\n",
    "1. **Fit Phase**:\n",
    "- Compute average rating and count for each item.\n",
    "- Filter items with too few ratings (to reduce noise).\n",
    "- Rank items by average rating to determine popularity.\n",
    "2. **Prediction**:\n",
    "- If the item exists in the popular list, return its average rating.\n",
    "- Otherwise, return the global average rating.\n",
    "3. **Recommendation**:\n",
    "- Return top-N popular items.\n",
    "- Optionally, exclude items the user has already seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "432ca8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityBaseline:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.popular_items = None\n",
    "        self.global_mean = None\n",
    "\n",
    "    def fit(self, ratings_df):\n",
    "        # Learns the most popular anime from the data\n",
    "\n",
    "        # Calculate the popularity score\n",
    "\n",
    "        item_stats = ratings_df.groupby('anime_id').agg({'rating' : ['mean','count']}).round(3)\n",
    "        item_stats.columns = ['avg_rating','rating_count']\n",
    "\n",
    "        # Only get anime with more than 10 ratings\n",
    "\n",
    "        item_stats = item_stats[item_stats['rating_count'] >= 10]\n",
    "\n",
    "        # Sort the anime by their avg_rating\n",
    "        self.popular_items = item_stats.sort_values('avg_rating', ascending = False)\n",
    "        self.global_mean = ratings_df['rating'].mean()\n",
    "\n",
    "        print(f\"Baseline trained on {len(item_stats)} popular items\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, user_id, item_id):\n",
    "        # Predicts the rating for a user-item pair\n",
    "        if self.popular_items is None:\n",
    "            return self.global_mean\n",
    "        \n",
    "        if item_id in self.popular_items.index:\n",
    "            return self.popular_items.loc[item_id, 'avg_rating']\n",
    "        else:\n",
    "            return self.global_mean\n",
    "        \n",
    "    def recommend(self, user_id, n_recommendations, exclude_seen = None):\n",
    "        # Recomend the top popular items\n",
    "\n",
    "        recommendations = self.popular_items.head(n_recommendations).index.tolist()\n",
    "\n",
    "        if exclude_seen:\n",
    "            # Get all the popular items which have not been seen by the user\n",
    "            recommendations = [item for item in recommendations if item not in exclude_seen]\n",
    "\n",
    "            remaining = n_recommendations - len(recommendations)\n",
    "            if remaining > 0:\n",
    "                additional = self.popular_items.iloc[n_recommendations:n_recommendations+remaining*2]\n",
    "                for item in additional.index:\n",
    "                    if item not in exclude_seen:\n",
    "                        recommendations.append(item)\n",
    "                        if len(recommendations) >= n_recommendations:\n",
    "                            break\n",
    "\n",
    "            return recommendations[:n_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0b509b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Popularity Baseline\n",
      "Baseline trained on 7364 popular items\n",
      " 1. GintamaÂ°                                 | Rating: 9.45 |Count: 1,182\n",
      " 1. Kimi no Na wa.                           | Rating: 9.42 |Count: 1,948\n",
      " 1. Ginga Eiyuu Densetsu                     | Rating: 9.39 |Count: 799\n",
      " 1. Fullmetal Alchemist: Brotherhood         | Rating: 9.32 |Count: 21,220\n",
      " 1. Gintama&#039;                            | Rating: 9.27 |Count: 3,098\n",
      " 1. Steins;Gate                              | Rating: 9.26 |Count: 17,019\n",
      " 1. Hunter x Hunter (2011)                   | Rating: 9.23 |Count: 7,418\n",
      " 1. Gintama                                  | Rating: 9.23 |Count: 4,222\n",
      " 1. Gintama&#039;: Enchousen                 | Rating: 9.20 |Count: 2,121\n",
      " 1. Gintama Movie: Kanketsu-hen - Yorozuya y | Rating: 9.19 |Count: 2,139\n"
     ]
    }
   ],
   "source": [
    "# Train baseline model\n",
    "print(\"Training Popularity Baseline\")\n",
    "baseline_model = PopularityBaseline()\n",
    "baseline_model.fit(ratings_filtered)\n",
    "\n",
    "# Show the top recommendation\n",
    "\n",
    "top_anime_ids = baseline_model.popular_items.head(10).index\n",
    "for i, anime_id in enumerate(top_anime_ids, 1):\n",
    "    anime_name = anime_clean[anime_clean['anime_id'] == anime_id]['name'].iloc[0]\n",
    "    avg_rating = baseline_model.popular_items.loc[anime_id, 'avg_rating']\n",
    "    rating_count = baseline_model.popular_items.loc[anime_id, 'rating_count']\n",
    "    print(f'{1:2d}. {anime_name[:40]:40} | Rating: {avg_rating:.2f} |Count: {rating_count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01b80f",
   "metadata": {},
   "source": [
    "## Content-Based Filtering for Anime Recommendations\n",
    "\n",
    "This section implements a **content-based recommendation system** that leverages **anime features** to generate personalised recommendations.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Feature Preparation**\n",
    "   - Genre features transformed using **TF-IDF**.\n",
    "   - Anime type features encoded as **one-hot vectors**.\n",
    "   - Numeric features like `rating`, `episodes`, and `members` **normalised**.\n",
    "   - Combined all features into a **single feature matrix**.\n",
    "\n",
    "2. **Similarity Computation**\n",
    "   - Compute **cosine similarity** between all anime.\n",
    "   - Store similarity matrix.\n",
    "\n",
    "3. **Personalized Recommendation**\n",
    "   - For each anime a user has rated, retrieve similar anime.\n",
    "   - Weight similarity scores by the userâs rating.\n",
    "   - Aggregate scores and sort to recommend the top-N anime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dfa3112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentBasedFilter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.item_features = None\n",
    "        self.item_similarity_matrix = None\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.anime_to_idx = None\n",
    "        self.idx_to_anime = None\n",
    "\n",
    "    def prepare_features(self, anime_df):\n",
    "        # preparing the feature matrix for anime\n",
    "\n",
    "\n",
    "        anime_ids = anime_df['anime_id'].values\n",
    "        self.anime_to_idx = {anime_id: idx for idx, anime_id in enumerate(anime_ids)}\n",
    "        self.idx_to_anime = {idx : anime_id for anime_id, idx in self.anime_to_idx.items()}\n",
    "\n",
    "        features_list = []\n",
    "\n",
    "        # Genres features using TF-IDF\n",
    "\n",
    "        genres = anime_df['genre'].fillna('Unknown')\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(max_features=50, stop_words=None)\n",
    "        genre_features = self.tfidf_vectorizer.fit_transform(genres).toarray()\n",
    "        features_list.append(genre_features)\n",
    "\n",
    "        # Type features\n",
    "\n",
    "        type_dummies = pd.get_dummies(anime_df['type']).values\n",
    "        features_list.append(type_dummies)\n",
    "\n",
    "        # Numerical features (normalised)\n",
    "\n",
    "        numerical_features = anime_df[['rating', 'episodes', 'members']].copy()\n",
    "        numerical_features['episodes'] = numerical_features['episodes'].fillna(1)\n",
    "\n",
    "        # normalise numerical features \n",
    "        for col in numerical_features.columns:\n",
    "            numerical_features[col] = (numerical_features[col] - numerical_features[col].mean()) / numerical_features[col].std()\n",
    "\n",
    "        features_list.append(numerical_features.values)\n",
    "\n",
    "        # Combine all features\n",
    "\n",
    "        self.item_features = np.hstack(features_list)\n",
    "\n",
    "        print(f\"Content features prepared: {self.item_features.shape}\")\n",
    "        return self\n",
    "    \n",
    "    def fit(self, anime_df):\n",
    "        # Train the content-based model\n",
    "\n",
    "        self.prepare_features(anime_df)\n",
    "\n",
    "        # Calculate the item similarity matrix\n",
    "\n",
    "        print(\"Calculating item simularity matrix\")\n",
    "        self.item_similarity_matrix = cosine_similarity(self.item_features)\n",
    "\n",
    "        print(\"Training complete\")\n",
    "        print(f\"Feature dimensions: {self.item_features.shape[1]}\")\n",
    "        print(f\"Items: {self.item_features.shape[0]}\")\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def get_silmilar_items(self, anime_id, n_similar=50):\n",
    "        # Get the most similar items to the anime given\n",
    "\n",
    "        if anime_id not in self.anime_to_idx:\n",
    "            return []\n",
    "        \n",
    "        item_idx = self.anime_to_idx[anime_id]\n",
    "        similarities = self.item_similarity_matrix[item_idx]\n",
    "\n",
    "        # Get indicies of the most similar items (except itself)\n",
    "\n",
    "        similar_indices = np.argsort(similarities)[::-1][1:n_similar+1]\n",
    "        similar_anime_ids = [self.idx_to_anime[idx] for idx in similar_indices]\n",
    "        similarity_scores = similarities[similar_indices]\n",
    "\n",
    "        return list(zip(similar_anime_ids, similarity_scores))\n",
    "    \n",
    "    def recommend(self, user_ratings, n_recommendations = 10):\n",
    "        # Recommend item based on the user's rating history\n",
    "        # user_ratings : dict {anime_id, rating}\n",
    "\n",
    "        if not user_ratings:\n",
    "            return []\n",
    "        \n",
    "        item_scores = defaultdict(float)\n",
    "        item_weights = defaultdict(float)\n",
    "\n",
    "        for rated_anime_id, rating in user_ratings.items():\n",
    "            if rated_anime_id not in self.anime_to_idx:\n",
    "                continue\n",
    "\n",
    "                # Get similar items to the given anime\n",
    "\n",
    "            similar_items = self.get_silmilar_items(rated_anime_id, n_similar=50)\n",
    "\n",
    "            for similar_anime_id, similarity in similar_items:\n",
    "                if similar_anime_id not in user_ratings: # Avoid recommending watched anime\n",
    "                    # Weight similarity by user's rating (higher rated items get more weight)\n",
    "                    weight = (rating - 5) * similarity \n",
    "                    item_scores[similar_anime_id] += item_weights\n",
    "                    item_weights[similar_anime_id] += abs(similarity)\n",
    "\n",
    "            # Calculate the final scores\n",
    "            final_scores = {}\n",
    "            for anime_id in item_scores:\n",
    "                if item_weights[anime_id] > 0:\n",
    "                       final_scores[anime_id] = item_scores[anime_id]/ item_weights[anime_id]\n",
    "\n",
    "            # Sort the final scores\n",
    "            recommendations = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            return [anime_id for anime_id, score in recommendations[:n_recommendations]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abaa2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training content-based filter\n",
      "Content features prepared: (10931, 56)\n",
      "Calculating item simularity matrix\n",
      "Training complete\n",
      "Feature dimensions: 56\n",
      "Items: 10931\n",
      "Finding similar anime to Boruto: Naruto the Movie\n",
      "1. The Last: Naruto the Movie               | Similarity 0.982\n",
      "2. Naruto: Shippuuden Movie 6 - Road to Nin | Similarity 0.982\n",
      "3. Naruto: Shippuuden Movie 4 - The Lost To | Similarity 0.976\n",
      "4. Naruto: Shippuuden Movie 3 - Hi no Ishi  | Similarity 0.975\n",
      "5. Dragon Ball Z Movie 14: Kami to Kami     | Similarity 0.972\n"
     ]
    }
   ],
   "source": [
    "# Training content-based filter\n",
    "print(\"Training content-based filter\")\n",
    "content_filter = ContentBasedFilter()\n",
    "content_filter.fit(anime_clean)\n",
    "\n",
    "# Example: find an anime similar to a popular one\n",
    "sample_anime_id = anime_clean.iloc[486]['anime_id']\n",
    "sample_anime_name = anime_clean.iloc[486]['name']\n",
    "\n",
    "print(f\"Finding similar anime to {sample_anime_name}\")\n",
    "similar_anime = content_filter.get_silmilar_items(sample_anime_id, n_similar=5)\n",
    "\n",
    "for i, (anime_id, similarity) in enumerate(similar_anime, 1):\n",
    "    anime_name = anime_clean[anime_clean['anime_id'] == anime_id]['name']\n",
    "    if not anime_name.empty:\n",
    "        print(f\"{i}. {anime_name.iloc[0][:40]:40} | Similarity {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc31397",
   "metadata": {},
   "source": [
    "# User-Based Collaborative Filtering\n",
    "\n",
    "Now that we have popularity and content-based approaches, we implement **Collaborative Filtering (CF)**, \n",
    "which leverages similarities between users to generate recommendations. \n",
    "\n",
    "### Key Steps:\n",
    "1. **User-Item Matrix Creation**  \n",
    "   - Construct a sparse matrix where each row corresponds to a user and each column corresponds to an anime.  \n",
    "   - Matrix entries represent ratings (0 if unrated).  \n",
    "   - Also generate index mappings for users and items.  \n",
    "\n",
    "2. **User-Based CF Algorithm**  \n",
    "   - Calculate similarity between users using **cosine similarity** on their overlapping ratings.  \n",
    "   - Predict ratings for unseen anime based on ratings from similar users.  \n",
    "   - Use weighted averages where more similar users have greater influence.  \n",
    "\n",
    "3. **Model Training & Recommendations**  \n",
    "   - Fit the CF model on the user-item matrix.  \n",
    "   - For a given user, recommend anime they havenât rated yet by predicting which ones they would likely rate highly.  \n",
    "   - Demonstrate recommendations for a sample user.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d902b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(rating_df):\n",
    "    # Creates a spase user_item rating matrix\n",
    "\n",
    "    # Creat mappings for matrix indices\n",
    "\n",
    "    user_ids = sorted(ratings_df['user_id'].unique())\n",
    "    item_ids = sorted(ratings_df['anime_id'].unique())\n",
    "\n",
    "    user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    item_to_idx = {item_id: idx for idx, item_id in enumerate(item_ids)}\n",
    "\n",
    "    # Create matrix\n",
    "\n",
    "    n_users, n_items = len(user_ids), len(item_ids)\n",
    "    matrix = np.zeros((n_users, n_items))\n",
    "\n",
    "    for _, row in ratings_df.iterrows():\n",
    "        user_idx = user_to_idx[row['user_id']]\n",
    "        item_idx = item_to_idx[row['anime_id']]\n",
    "        matrix[user_idx, item_idx] = row['rating']\n",
    "\n",
    "    return matrix, user_to_idx, item_to_idx, user_ids, item_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a86cbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user-item matrix\n",
      "User-item matrix shape: (73515, 11200)\n",
      "Non-zero entries: 7,813,730\n",
      "Sparsity: 99.05%\n"
     ]
    }
   ],
   "source": [
    "# Creating sample matrix\n",
    "print('Creating user-item matrix')\n",
    "\n",
    "sample_size = 10000\n",
    "ratings_sample = ratings_filtered.sample(n=min(sample_size, len(ratings_filtered)), random_state=42)\n",
    "user_item_matrix, user_to_index, item_to_index, user_ids, item_ids = create_user_item_matrix(ratings_sample)\n",
    "\n",
    "print(f\"User-item matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"Non-zero entries: {np.count_nonzero(user_item_matrix):,}\")\n",
    "print(f\"Sparsity: {(1 - np.count_nonzero(user_item_matrix) / user_item_matrix.size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "280d5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBasedCF:\n",
    "    # User-based collaborative filtering\n",
    "\n",
    "    def __init__(self, k = 20, similarity_threshold = 0.1):\n",
    "        self.k = k # number of similar users to consider\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.user_item_matrix = None\n",
    "        self.user_similarities = None\n",
    "        self.user_means = None\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        # Train the model on user-item matrix\n",
    "\n",
    "        self.user_item_matrix = user_item_matrix\n",
    "\n",
    "        # Calculate the user means (for mean-centered ratings)\n",
    "\n",
    "        self.user_means = np.array([user_ratings[user_ratings > 0].mean() if np.any(user_ratings > 0) else 0 for user_ratings in user_item_matrix])\n",
    "\n",
    "        print(\"User-based CF model trained\")\n",
    "        print(f\"Matrix shape: {user_item_matrix.shape}\")\n",
    "        print(f\"Using k={self.k} similar users\")\n",
    "\n",
    "    def get_user_similarity(self, user1_idx, user2_idx):\n",
    "        # Calculate cosine similarity between two users\n",
    "\n",
    "        user1_ratings = self.user_item_matrix[user1_idx]\n",
    "        user2_ratings = self.user_item_matrix[user2_idx]\n",
    "\n",
    "        # Find common rated items \n",
    "\n",
    "        common_items = (user1_ratings > 0) & (user2_ratings > 0)\n",
    "\n",
    "        if np.sum(common_items) < 2:\n",
    "            return 0\n",
    "        \n",
    "        user1_common = user1_ratings[common_items]\n",
    "        user2_common = user2_ratings[common_items]\n",
    "\n",
    "        # Calculate cosine simularity\n",
    "\n",
    "        dot_product = np.dot(user1_common, user2_common)\n",
    "        norm1 = np.linalg.norm(user1_common)\n",
    "        norm2 = np.linalg.norm(user2_common)\n",
    "\n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            return 0\n",
    "        \n",
    "        return dot_product / (norm1 * norm2)\n",
    "    \n",
    "\n",
    "    def predict(self, user_idx, item_idx):\n",
    "        # Predict rating for the user-item pairs\n",
    "\n",
    "        if self.user_item_matrix[user_idx, item_idx] > 0:\n",
    "            # User has already rated this anime\n",
    "            return self.user_item_matrix[user_idx, item_idx]\n",
    "        \n",
    "        similarites = []\n",
    "        ratings = []\n",
    "\n",
    "        for other_user_idx in range(len(self.user_item_matrix)):\n",
    "            if other_user_idx == user_idx:\n",
    "                continue\n",
    "\n",
    "            if self.user_item_matrix[other_user_idx, item_idx] > 0:\n",
    "                sim = self.get_user_similarity(user_idx, other_user_idx)\n",
    "                if sim > self.similarity_threshold:\n",
    "                    similarites.append(sim)\n",
    "                    ratings.append(self.user_item_matrix[other_user_idx,item_idx])\n",
    "\n",
    "        if not similarites:\n",
    "            return self.user_means[user_idx] if self.user_means[user_idx] > 0 else 3.0\n",
    "        \n",
    "        # Wighted average prediction\n",
    "\n",
    "        similarites = np.array(similarites)\n",
    "        ratings = np.array(ratings)\n",
    "\n",
    "        if np.sum(similarites) == 0:\n",
    "            return self.user_means[user_idx] if self.user_means[user_idx] > 0 else 3.0\n",
    "        \n",
    "        predicted_rating = np.sum(similarites * ratings) / np.sum(similarites)\n",
    "        return np.clip(predicted_rating, 1, 10) # Ensure the rating is in a valid range\n",
    "    \n",
    "    def recommend(self, user_idx, n_recommendations=10, exclude_seen=True):\n",
    "        # Recommend item for a user\n",
    "\n",
    "        user_ratings = self.user_item_matrix[user_idx]\n",
    "        unrated_items = np.where(user_ratings == 0)[0]\n",
    "\n",
    "        if len(unrated_items) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Predict ratings for unrated items\n",
    "\n",
    "        predictions = []\n",
    "        for item_idx in unrated_items:\n",
    "            pred_rating = self.predict(user_idx, item_idx)\n",
    "            predictions.append((item_idx, pred_rating))\n",
    "\n",
    "        # Sort by predicted rating and return top N\n",
    "\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [item_idx for item_idx, _ in predictions[:n_recommendations]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b395c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training user-based collaborative filtering\n",
      "User-based CF model trained\n",
      "Matrix shape: (73515, 11200)\n",
      "Using k=20 similar users\n",
      "Sample recommendations for User : 1:\n",
      "1. Nurse Angel Ririka SOS                   | Predicted Rating: 10.00\n",
      "2. Kingyo Chuuihou!                         | Predicted Rating: 10.00\n",
      "3. Oishinbo                                 | Predicted Rating: 10.00\n",
      "4. Future GPX Cyber Formula: Early Days Ren | Predicted Rating: 10.00\n",
      "5. PostPet Momobin                          | Predicted Rating: 10.00\n"
     ]
    }
   ],
   "source": [
    "# Training user-based CF\n",
    "print(\"Training user-based collaborative filtering\")\n",
    "user_cf = UserBasedCF(k=20)\n",
    "user_cf.fit(user_item_matrix)\n",
    "\n",
    "# Example recommendation\n",
    "\n",
    "test_user_idx = 0\n",
    "if test_user_idx < len(user_item_matrix):\n",
    "    print(f\"Sample recommendations for User : {user_ids[test_user_idx]}:\")\n",
    "    recommendations = user_cf.recommend(test_user_idx, n_recommendations=5)\n",
    "\n",
    "    for i, item_idx in enumerate(recommendations,1):\n",
    "        item_id = item_ids[item_idx]\n",
    "        anime_name = anime_clean[anime_clean[\"anime_id\"] == item_id]['name']\n",
    "        if not anime_name.empty:\n",
    "            predicted_rating = user_cf.predict(test_user_idx, item_idx)\n",
    "            print(f\"{i}. {anime_name.iloc[0][:40]:40} | Predicted Rating: {predicted_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b65033",
   "metadata": {},
   "source": [
    "## Hybrid Recommendation System â Matrix Factorisation (SVD)\n",
    "\n",
    "Matrix Factorisation is a latent-factor approach that reduces the high-dimensional userâitem rating matrix into a smaller set of **hidden features** that capture patterns in user preferences and item attributes.\n",
    "\n",
    "### Key Idea\n",
    "Instead of relying only on observed similarities (like CF) or metadata (like content-based), this method learns a **compressed representation** of both users and items:\n",
    "- **Users** are mapped into a latent feature space based on their rating behavior.\n",
    "- **Items** are mapped into the same latent space based on who rated them and how.\n",
    "- The interaction between a userâs vector and an itemâs vector predicts ratings.\n",
    "\n",
    "### Steps in Implementation\n",
    "1. **Global Mean Centering**  \n",
    "   - Calculate the average rating across the dataset.  \n",
    "   - Subtract this mean from observed ratings to focus on relative preferences.\n",
    "\n",
    "2. **Truncated SVD**  \n",
    "   - Decompose the userâitem matrix into:  \n",
    "     - `user_factors` â low-dimensional user representations.  \n",
    "     - `item_factors` â low-dimensional item representations.  \n",
    "\n",
    "3. **Prediction**  \n",
    "   - For user *u* and item *i*, the rating is estimated as:  \n",
    "     \\[\n",
    "     \\hat{r}_{ui} = \\mu + U_u \\cdot V_i\n",
    "     \\]  \n",
    "     Where \\(\\mu\\) is the global mean rating, and \\(U_u, V_i\\) are latent vectors.\n",
    "\n",
    "4. **Recommendation**  \n",
    "   - Predict scores for all unrated items.  \n",
    "   - Rank them and recommend the top-N highest scoring items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08a1db97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorisationSVD:\n",
    "    # Matrix Factorisation using Truncated SVD\n",
    "\n",
    "    def __init__(self, n_components = 50):\n",
    "        self.n_components = n_components\n",
    "        self.svd = None\n",
    "        self.user_factors = None\n",
    "        self.item_factors = None\n",
    "        self.global_mean = None\n",
    "\n",
    "    def fit(self, user_item_matrix):\n",
    "        # Training the SVD model \n",
    "\n",
    "        print(f\"Training matrix factorisation (SVD) with {self.n_components} components\")\n",
    "\n",
    "        self.global_mean = user_item_matrix[user_item_matrix > 0].mean()\n",
    "\n",
    "        # Center centered matrix (only for non-zero entries)\n",
    "\n",
    "        centered_matrix = user_item_matrix.copy()\n",
    "        mask = user_item_matrix > 0\n",
    "        centered_matrix[mask] -= self.global_mean\n",
    "\n",
    "        # Apply SVD\n",
    "\n",
    "        self.svd = TruncatedSVD(n_components=self.n_components, random_state=42)\n",
    "        self.user_factors = self.svd.fit_transform(centered_matrix)\n",
    "\n",
    "        # Get item factors\n",
    "\n",
    "        self.item_factors = self.svd.components_.T\n",
    "\n",
    "        print(f\"SVD model trained\")\n",
    "        print(f\"Explained variance ratio: {self.svd.explained_variance_ratio_.sum():.3f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, user_idx, item_idx):\n",
    "        # Predict the rating for user-item pairs\n",
    "\n",
    "        prediction = self.global_mean + np.dot(self.user_factors[user_idx], self.item_factors[item_idx])\n",
    "        return np.clip(prediction, 1, 10)\n",
    "    \n",
    "    def recommend(self, user_idx, n_recommendations=10, exclude_seen=True):\n",
    "        # Recommend items for a user\n",
    "        # Calculate predictions for all items\n",
    "\n",
    "        user_vector = self.user_factors[user_idx]\n",
    "        predictions = self.global_mean + np.dot(self.item_factors, user_vector)\n",
    "\n",
    "        # Gret indices of top predictions\n",
    "\n",
    "        top_indices = np.argsort(predictions)[::-1]\n",
    "\n",
    "        recommendations = []\n",
    "\n",
    "        for item_idx in top_indices:\n",
    "            if len(recommendations) >= n_recommendations:\n",
    "                break\n",
    "\n",
    "            # Exclude items the user has already seen\n",
    "            if exclude_seen and user_item_matrix[user_idx, item_idx] > 0:\n",
    "                continue\n",
    "\n",
    "            recommendations.append(item_idx)\n",
    "\n",
    "        return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2256178b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Matrix Factorisation Mode\n",
      "Training matrix factorisation (SVD) with 50 components\n",
      "SVD model trained\n",
      "Explained variance ratio: 0.243\n",
      "SVD Recommendation for User 1:\n",
      "1. Highschool of the Dead: Drifters of the  | Predicted Rating: 8.15\n",
      "2. Mahou Shoujo MadokaâMagica               | Predicted Rating: 8.13\n",
      "3. K-On!                                    | Predicted Rating: 8.10\n",
      "4. Hellsing                                 | Predicted Rating: 8.09\n",
      "5. Bishoujo Senshi Sailor Moon              | Predicted Rating: 8.06\n"
     ]
    }
   ],
   "source": [
    "# Training SVD model \n",
    "\n",
    "print(\"Training Matrix Factorisation Mode\")\n",
    "svd_model = MatrixFactorisationSVD(n_components=50)\n",
    "svd_model.fit(user_item_matrix)\n",
    "\n",
    "# Example recommendations\n",
    "\n",
    "if test_user_idx < len(user_item_matrix):\n",
    "    print(f\"SVD Recommendation for User {user_ids[test_user_idx]}:\")\n",
    "    svd_recommendations = svd_model.recommend(test_user_idx, n_recommendations = 5)\n",
    "    for i, item_idx in enumerate(svd_recommendations, 1):\n",
    "        item_id = item_ids[item_idx]\n",
    "        anime_name = anime_clean[anime_clean['anime_id'] == item_id]['name']\n",
    "        if not anime_name.empty:\n",
    "            predicted_rating = svd_model.predict(test_user_idx, item_idx)\n",
    "            print(f\"{i}. {anime_name.iloc[0][:40]:40} | Predicted Rating: {predicted_rating:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86023b1",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To assess the performance of our recommendation models, we split the dataset into **train** and **test sets**.  \n",
    "We use a **temporal split**, ensuring that for each user, their most recent ratings form the test set (20% of their ratings).  \n",
    "\n",
    "We evaluate the models using two types of metrics:\n",
    "\n",
    "- **Rating Prediction Metrics**\n",
    "  - **RMSE (Root Mean Squared Error):** Measures the average difference between predicted and actual ratings. Lower is better.\n",
    "  - **MAE (Mean Absolute Error):** Measures the absolute average difference. Lower is better.\n",
    "\n",
    "- **Recommendation Quality Metrics**\n",
    "  - **Precision@K:** Proportion of recommended items that are relevant.\n",
    "  - **Recall@K:** Proportion of relevant items that were recommended.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6dd39cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_temporal(ratings_df, test_ratio = 0.2):\n",
    "    # Split the data by time (the last 20% of each users ratings as test)\n",
    "\n",
    "    train_data = []\n",
    "    test_data = []\n",
    "\n",
    "    for user_id in ratings_df['user_id'].unique():\n",
    "        user_ratings = ratings_df[ratings_df['user_id'] == user_id].sort_values('anime_id')\n",
    "        n_ratings = len(user_ratings)\n",
    "        n_test = max(1, int(n_ratings * test_ratio))\n",
    "\n",
    "        train_data.append(user_ratings.iloc[:-n_test])\n",
    "        test_data.append(user_ratings.iloc[-n_test:])\n",
    "\n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    test_df = pd.concat(test_data, ignore_index=True)\n",
    "    \n",
    "    return train_df, test_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090c1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred):\n",
    "    \"\"\"Calculate RMSE and MAE\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4c6f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalaute_recommendations(model, test_data, user_to_index, item_to_index, k = 10):\n",
    "    # Evaluate the quality of the recommendations\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for user_id in test_data['user_id'].unique():\n",
    "        if user_id not in user_to_index:\n",
    "            continue\n",
    "\n",
    "        user_idx = user_to_index[user_id]\n",
    "        user_test_items = set(test_data[test_data['user_id'] == user_id]['anime_id'.values])\n",
    "\n",
    "        try:\n",
    "            recommendations = model.recommend(user_idx, n_recommendations = k)\n",
    "            if hasattr(model, 'recommend') and recommendations:\n",
    "                # Convert the indices to item IDs if needed\n",
    "\n",
    "                if isinstance(recommendations[0], int) and recommendations[0] < len(item_ids):\n",
    "                    recommended_items = set(items_ids[idx] for idx in recommendations)\n",
    "                else:\n",
    "                    recommended_items = set(recommendations)\n",
    "\n",
    "                relevent_items = user_test_items.intersection(recommendations)\n",
    "\n",
    "                precision = len(relevent_items) / len(recommended_items) if recommended_items else 0\n",
    "                recall = len(relevent_items) / len(user_test_items) if user_test_items else 0\n",
    "\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    \n",
    "    return avg_precision, avg_recall\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
